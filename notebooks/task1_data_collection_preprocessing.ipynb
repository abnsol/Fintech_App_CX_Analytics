{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e43298ff",
   "metadata": {},
   "source": [
    "# Task 1: Data Collection and Preprocessing for Fintech App Reviews\n",
    "\n",
    "This notebook covers the first task of the Week 2 challenge: scraping user reviews from the Google Play Store for three Ethiopian banks (Commercial Bank of Ethiopia, Bank of Abyssinia, and Dashen Bank), followed by essential data preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7db61b4",
   "metadata": {},
   "source": [
    "## 1. Setup and Library Imports\n",
    "\n",
    "First, let's install and import the necessary libraries.\n",
    "If you haven't already, ensure these are installed via your `requirements.txt` as we discussed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c63582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google_play_scraper import Sort, reviews_all\n",
    "from datetime import datetime\n",
    "import os \n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfe632e",
   "metadata": {},
   "source": [
    "## 2. Define App Package Names\n",
    "\n",
    "We need the unique package names (app IDs) for each bank's mobile application on the Google Play Store. These were identified through prior search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da978b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_packages = {\n",
    "    'Commercial Bank of Ethiopia': 'com.combanketh.mobilebanking',\n",
    "    'Bank of Abyssinia': 'com.boa.boaMobileBanking',\n",
    "    'Dashen Bank': 'com.dashen.dashensuperapp'\n",
    "}\n",
    "\n",
    "print(\"App package names defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9818ef3",
   "metadata": {},
   "source": [
    "## 3. Web Scraping Google Play Store Reviews\n",
    "\n",
    "We will use the `google-play-scraper` library to collect reviews.\n",
    "The goal is to collect at least 400 reviews per bank, totaling 1200+ reviews.\n",
    "We will collect review text, rating, and date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c810a0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews = []\n",
    "\n",
    "for bank_name, app_id in app_packages.items():\n",
    "    print(f\"Scraping reviews for {bank_name} (App ID: {app_id})...\")\n",
    "    try:\n",
    "        # Scrape reviews\n",
    "        # Collecting more than 400 to ensure we have enough after cleaning\n",
    "        result, continuation_token = reviews_all(\n",
    "            app_id,\n",
    "            lang='en', # English reviews\n",
    "            country='et', # From Ethiopia\n",
    "            sort=Sort.NEWEST, # Get the newest reviews\n",
    "            count=1000 # Attempt to get up to 1000 reviews per app to ensure 400+ after cleaning\n",
    "        )\n",
    "        for r in result:\n",
    "            all_reviews.append({\n",
    "                'review_id': r['reviewId'],\n",
    "                'bank': bank_name,\n",
    "                'app_id': app_id,\n",
    "                'review_text': r['content'],\n",
    "                'rating': r['score'],\n",
    "                'date': r['at'],\n",
    "                'source': 'Google Play'\n",
    "            })\n",
    "        print(f\"Successfully scraped {len(result)} reviews for {bank_name}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {bank_name}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the collected reviews\n",
    "df_reviews = pd.DataFrame(all_reviews)\n",
    "print(f\"\\nTotal raw reviews collected: {len(df_reviews)}\")\n",
    "print(\"Sample of raw data:\")\n",
    "print(df_reviews.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b834a6",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "\n",
    "This section handles data cleaning, including removing duplicates, handling missing data, and normalizing dates.\n",
    "\n",
    "### 4.1 Remove Duplicates\n",
    "\n",
    "Remove duplicate reviews based on `review_id` and `review_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1b8db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_cleaned = df_reviews.drop_duplicates(subset=['review_id', 'review_text']).copy()\n",
    "print(f\"\\nReviews after removing duplicates: {len(df_reviews_cleaned)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7001054c",
   "metadata": {},
   "source": [
    "### 4.2 Handle Missing Data\n",
    "\n",
    "Check for missing values and remove/fill them as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d238615",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMissing values before handling:\")\n",
    "print(df_reviews_cleaned.isnull().sum())\n",
    "\n",
    "# Remove rows where `review_text` or `rating` is missing, as these are critical for analysis.\n",
    "df_reviews_cleaned.dropna(subset=['review_text', 'rating'], inplace=True)\n",
    "print(f\"Reviews after dropping rows with missing critical data: {len(df_reviews_cleaned)}\")\n",
    "\n",
    "# Fill any other potential missing values if necessary (e.g., empty string for text, 'Unknown' for source)\n",
    "df_reviews_cleaned['review_text'].fillna('', inplace=True)\n",
    "df_reviews_cleaned['source'].fillna('Unknown', inplace=True)\n",
    "\n",
    "print(\"\\nMissing values after handling:\")\n",
    "print(df_reviews_cleaned.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46344a35",
   "metadata": {},
   "source": [
    "### 4.3 Normalize Dates\n",
    "\n",
    "Convert the 'date' column to datetime objects and then format to `YYYY-MM-DD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307ebb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_cleaned['date'] = pd.to_datetime(df_reviews_cleaned['date'])\n",
    "df_reviews_cleaned['date'] = df_reviews_cleaned['date'].dt.strftime('%Y-%m-%d')\n",
    "print(\"\\nDates normalized to YYYY-MM-DD format.\")\n",
    "print(\"Sample of cleaned data dates:\")\n",
    "print(df_reviews_cleaned['date'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0358647b",
   "metadata": {},
   "source": [
    "### 4.4 Ensure Required Columns and Order\n",
    "\n",
    "Ensure the final DataFrame has the required columns: `review`, `rating`, `date`, `bank`, `source`.\n",
    "Rename 'review_text' to 'review' for consistency with requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afe8369",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_cleaned.rename(columns={'review_text': 'review'}, inplace=True)\n",
    "\n",
    "# Select and reorder columns\n",
    "final_columns = ['review_id', 'review', 'rating', 'date', 'bank', 'source']\n",
    "df_reviews_cleaned = df_reviews_cleaned[final_columns]\n",
    "\n",
    "print(\"\\nFinal cleaned DataFrame structure:\")\n",
    "print(df_reviews_cleaned.head())\n",
    "print(f\"\\nTotal cleaned reviews: {len(df_reviews_cleaned)}\")\n",
    "\n",
    "# Check if we met the 1200+ reviews target\n",
    "min_reviews_met = len(df_reviews_cleaned) >= 1200\n",
    "print(f\"\\nMet minimum 1200 reviews target: {min_reviews_met}\")\n",
    "if not min_reviews_met:\n",
    "    print(\"Warning: Did not meet the minimum 1200 reviews target. Consider increasing 'count' in scraping or reviewing app IDs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94466cf8",
   "metadata": {},
   "source": [
    "## 5. Save Cleaned Data\n",
    "\n",
    "Save the cleaned DataFrame to a CSV file in the `data/processed/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b32b20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the data/processed directory exists\n",
    "output_dir = '../data/processed/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(output_dir, 'google_play_reviews_cleaned.csv')\n",
    "df_reviews_cleaned.to_csv(output_path, index=False)\n",
    "print(f\"\\nCleaned data saved to {output_path}\")\n",
    "\n",
    "# Verify the saved file\n",
    "try:\n",
    "    df_check = pd.read_csv(output_path)\n",
    "    print(\"\\nVerification: Successfully loaded the saved CSV.\")\n",
    "    print(df_check.info())\n",
    "except Exception as e:\n",
    "    print(f\"\\nVerification failed: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
